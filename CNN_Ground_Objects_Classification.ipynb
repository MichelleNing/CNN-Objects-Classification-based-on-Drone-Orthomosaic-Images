{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Ground Objects_Classification\n",
    "\n",
    "## Project: Write an Algorithm for Ground Objects Classification\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, convolutional neural networks will be used to build an algorithm for ground objects classification based on drone orthomaic images. There are 5 different classes: CropA, CropB, Tree, Hay, and Building. In data folder, Dr. Jili Li offered images, with size: 375 x 375.\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "Currently, in this real-world, the accuracy gournd objects classification based on drone orthomaic images is around 70% to 80%. \n",
    "\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "Here, break the notebook into separate steps.\n",
    "\n",
    "* [Step 1](#step1): Import Datasets\n",
    "* [Step 2](#step2): Create a Model to Classify objects\n",
    "* [Step 3](#step3): Train the Model\n",
    "* [Step 4](#step4): Test the Model\n",
    "\n",
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Import Datasets\n",
    "\n",
    "All images are in .tif format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 284 total images.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# load filenames\n",
    "image_files = np.array(glob(\"data/*/*/*.tif\"))\n",
    "# print number of images in dataset\n",
    "print('There are %d total images.' % len(image_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "data_dir = 'data/'\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "valid_dir = os.path.join(data_dir, 'valid/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# data transform for training\n",
    "data_transform_train = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                           transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.RandomResizedCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# data transform for validation and tests\n",
    "data_transform = transforms.Compose([transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform_train)\n",
    "valid_data = datasets.ImageFolder(valid_dir, transform=data_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 30\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "loaders_scratch = {'train': train_loader, 'valid': valid_loader,\n",
    "                  'test': test_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Create a Model to Classify objects\n",
    "\n",
    "Here, use Convolutional Neural Network to build model.\n",
    "It includes three Convolutional layers, each of them followed by a pooling layer.\n",
    "After convolutional layers, there are two fully connected hidden layers.\n",
    "Drop out layers are also adopted in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### create an architecture\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## Define layers of a CNN\n",
    "        # input size = 224 * 224 * 3 image tensor\n",
    "        self.conv_1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        # 112 * 112 * 8\n",
    "        self.conv_2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        # 56 * 56 * 16\n",
    "        self.conv_3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 133)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "        x = self.pool(F.relu(self.conv_1(x)))\n",
    "        x = self.pool(F.relu(self.conv_2(x)))\n",
    "        x = self.pool(F.relu(self.conv_3(x)))\n",
    "        \n",
    "        x = x.view(-1, 32*28*28)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# instantiate the CNN\n",
    "model_scratch = Net()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model_scratch.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Train the Model\n",
    "\n",
    "Before train the model, first set criterion and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "### set loss function\n",
    "criterion_scratch = nn.CrossEntropyLoss()\n",
    "\n",
    "### set optimizer\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ## record the average training loss\n",
    "            train_loss += (1/(batch_idx + 1)) * (loss.item() - train_loss)\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            \n",
    "        valid_loss = valid_loss/len(loaders['valid'].sampler)\n",
    "\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss    \n",
    "    # return trained model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 4.862064 \tValidation Loss: 4.797994\n",
      "Validation loss decreased (inf --> 4.797994).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 4.759767 \tValidation Loss: 4.675023\n",
      "Validation loss decreased (4.797994 --> 4.675023).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 4.613300 \tValidation Loss: 4.456644\n",
      "Validation loss decreased (4.675023 --> 4.456644).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 4.300485 \tValidation Loss: 3.914404\n",
      "Validation loss decreased (4.456644 --> 3.914404).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 3.472538 \tValidation Loss: 2.531382\n",
      "Validation loss decreased (3.914404 --> 2.531382).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 2.205625 \tValidation Loss: 1.920668\n",
      "Validation loss decreased (2.531382 --> 1.920668).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.949517 \tValidation Loss: 1.806087\n",
      "Validation loss decreased (1.920668 --> 1.806087).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.789902 \tValidation Loss: 1.763120\n",
      "Validation loss decreased (1.806087 --> 1.763120).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.734606 \tValidation Loss: 1.735903\n",
      "Validation loss decreased (1.763120 --> 1.735903).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.652718 \tValidation Loss: 1.661515\n",
      "Validation loss decreased (1.735903 --> 1.661515).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.648966 \tValidation Loss: 1.595474\n",
      "Validation loss decreased (1.661515 --> 1.595474).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.603077 \tValidation Loss: 1.521196\n",
      "Validation loss decreased (1.595474 --> 1.521196).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.723829 \tValidation Loss: 1.641654\n",
      "Epoch: 14 \tTraining Loss: 1.559613 \tValidation Loss: 1.541675\n",
      "Epoch: 15 \tTraining Loss: 1.561215 \tValidation Loss: 1.677582\n",
      "Epoch: 16 \tTraining Loss: 1.665609 \tValidation Loss: 1.587749\n",
      "Epoch: 17 \tTraining Loss: 1.600162 \tValidation Loss: 1.486656\n",
      "Validation loss decreased (1.521196 --> 1.486656).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.539744 \tValidation Loss: 1.660031\n",
      "Epoch: 19 \tTraining Loss: 1.587044 \tValidation Loss: 1.655906\n",
      "Epoch: 20 \tTraining Loss: 1.538255 \tValidation Loss: 1.669394\n",
      "Epoch: 21 \tTraining Loss: 1.503989 \tValidation Loss: 1.487284\n",
      "Epoch: 22 \tTraining Loss: 1.489168 \tValidation Loss: 1.561073\n",
      "Epoch: 23 \tTraining Loss: 1.500541 \tValidation Loss: 1.472854\n",
      "Validation loss decreased (1.486656 --> 1.472854).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.453483 \tValidation Loss: 1.421112\n",
      "Validation loss decreased (1.472854 --> 1.421112).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.467837 \tValidation Loss: 1.442151\n",
      "Epoch: 26 \tTraining Loss: 1.397348 \tValidation Loss: 1.447072\n",
      "Epoch: 27 \tTraining Loss: 1.320984 \tValidation Loss: 1.397201\n",
      "Validation loss decreased (1.421112 --> 1.397201).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.391838 \tValidation Loss: 1.344635\n",
      "Validation loss decreased (1.397201 --> 1.344635).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.360187 \tValidation Loss: 1.661781\n",
      "Epoch: 30 \tTraining Loss: 1.344901 \tValidation Loss: 1.345355\n",
      "Epoch: 31 \tTraining Loss: 1.303279 \tValidation Loss: 1.276865\n",
      "Validation loss decreased (1.344635 --> 1.276865).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.209857 \tValidation Loss: 1.303910\n",
      "Epoch: 33 \tTraining Loss: 1.183386 \tValidation Loss: 1.233907\n",
      "Validation loss decreased (1.276865 --> 1.233907).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.125535 \tValidation Loss: 1.316718\n",
      "Epoch: 35 \tTraining Loss: 1.118302 \tValidation Loss: 1.225928\n",
      "Validation loss decreased (1.233907 --> 1.225928).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.124390 \tValidation Loss: 1.195257\n",
      "Validation loss decreased (1.225928 --> 1.195257).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.086305 \tValidation Loss: 1.103512\n",
      "Validation loss decreased (1.195257 --> 1.103512).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.027054 \tValidation Loss: 1.086227\n",
      "Validation loss decreased (1.103512 --> 1.086227).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.003861 \tValidation Loss: 1.289806\n",
      "Epoch: 40 \tTraining Loss: 0.949445 \tValidation Loss: 1.106452\n",
      "Epoch: 41 \tTraining Loss: 0.918776 \tValidation Loss: 1.080719\n",
      "Validation loss decreased (1.086227 --> 1.080719).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.867900 \tValidation Loss: 1.038721\n",
      "Validation loss decreased (1.080719 --> 1.038721).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.819275 \tValidation Loss: 1.050675\n",
      "Epoch: 44 \tTraining Loss: 0.743601 \tValidation Loss: 1.078734\n",
      "Epoch: 45 \tTraining Loss: 0.817453 \tValidation Loss: 0.977079\n",
      "Validation loss decreased (1.038721 --> 0.977079).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.791530 \tValidation Loss: 1.010340\n",
      "Epoch: 47 \tTraining Loss: 0.770272 \tValidation Loss: 0.953056\n",
      "Validation loss decreased (0.977079 --> 0.953056).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.741283 \tValidation Loss: 0.919913\n",
      "Validation loss decreased (0.953056 --> 0.919913).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.659987 \tValidation Loss: 0.881811\n",
      "Validation loss decreased (0.919913 --> 0.881811).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.674156 \tValidation Loss: 0.867529\n",
      "Validation loss decreased (0.881811 --> 0.867529).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.630688 \tValidation Loss: 0.891622\n",
      "Epoch: 52 \tTraining Loss: 0.667487 \tValidation Loss: 1.018692\n",
      "Epoch: 53 \tTraining Loss: 0.768808 \tValidation Loss: 0.853811\n",
      "Validation loss decreased (0.867529 --> 0.853811).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.556521 \tValidation Loss: 0.784767\n",
      "Validation loss decreased (0.853811 --> 0.784767).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.592058 \tValidation Loss: 0.832155\n",
      "Epoch: 56 \tTraining Loss: 0.501902 \tValidation Loss: 0.856923\n",
      "Epoch: 57 \tTraining Loss: 0.492672 \tValidation Loss: 0.827994\n",
      "Epoch: 58 \tTraining Loss: 0.653317 \tValidation Loss: 0.737295\n",
      "Validation loss decreased (0.784767 --> 0.737295).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.585978 \tValidation Loss: 0.884667\n",
      "Epoch: 60 \tTraining Loss: 0.579059 \tValidation Loss: 0.738573\n",
      "Epoch: 61 \tTraining Loss: 0.572498 \tValidation Loss: 0.905596\n",
      "Epoch: 62 \tTraining Loss: 0.510932 \tValidation Loss: 0.712719\n",
      "Validation loss decreased (0.737295 --> 0.712719).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.521268 \tValidation Loss: 0.684469\n",
      "Validation loss decreased (0.712719 --> 0.684469).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.528239 \tValidation Loss: 0.664640\n",
      "Validation loss decreased (0.684469 --> 0.664640).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.480835 \tValidation Loss: 0.734672\n",
      "Epoch: 66 \tTraining Loss: 0.502478 \tValidation Loss: 0.905411\n",
      "Epoch: 67 \tTraining Loss: 0.546282 \tValidation Loss: 0.598714\n",
      "Validation loss decreased (0.664640 --> 0.598714).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.504835 \tValidation Loss: 0.639069\n",
      "Epoch: 69 \tTraining Loss: 0.468409 \tValidation Loss: 0.735097\n",
      "Epoch: 70 \tTraining Loss: 0.480956 \tValidation Loss: 0.645525\n",
      "Epoch: 71 \tTraining Loss: 0.440099 \tValidation Loss: 0.702680\n",
      "Epoch: 72 \tTraining Loss: 0.454337 \tValidation Loss: 0.708825\n",
      "Epoch: 73 \tTraining Loss: 0.548027 \tValidation Loss: 0.564048\n",
      "Validation loss decreased (0.598714 --> 0.564048).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.476488 \tValidation Loss: 0.744955\n",
      "Epoch: 75 \tTraining Loss: 0.418112 \tValidation Loss: 0.753584\n",
      "Epoch: 76 \tTraining Loss: 0.450604 \tValidation Loss: 0.620977\n",
      "Epoch: 77 \tTraining Loss: 0.437993 \tValidation Loss: 0.742196\n",
      "Epoch: 78 \tTraining Loss: 0.434574 \tValidation Loss: 0.646344\n",
      "Epoch: 79 \tTraining Loss: 0.469917 \tValidation Loss: 0.673078\n",
      "Epoch: 80 \tTraining Loss: 0.464543 \tValidation Loss: 1.206425\n",
      "Epoch: 81 \tTraining Loss: 0.499132 \tValidation Loss: 0.554278\n",
      "Validation loss decreased (0.564048 --> 0.554278).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.551412 \tValidation Loss: 0.582518\n",
      "Epoch: 83 \tTraining Loss: 0.388514 \tValidation Loss: 0.649248\n",
      "Epoch: 84 \tTraining Loss: 0.422447 \tValidation Loss: 0.566452\n",
      "Epoch: 85 \tTraining Loss: 0.387480 \tValidation Loss: 0.624845\n",
      "Epoch: 86 \tTraining Loss: 0.388243 \tValidation Loss: 0.567904\n",
      "Epoch: 87 \tTraining Loss: 0.383679 \tValidation Loss: 0.550578\n",
      "Validation loss decreased (0.554278 --> 0.550578).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.427553 \tValidation Loss: 0.638671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 \tTraining Loss: 0.343710 \tValidation Loss: 0.568770\n",
      "Epoch: 90 \tTraining Loss: 0.388944 \tValidation Loss: 0.551098\n",
      "Epoch: 91 \tTraining Loss: 0.361023 \tValidation Loss: 0.608087\n",
      "Epoch: 92 \tTraining Loss: 0.399821 \tValidation Loss: 0.554086\n",
      "Epoch: 93 \tTraining Loss: 0.348459 \tValidation Loss: 0.524965\n",
      "Validation loss decreased (0.550578 --> 0.524965).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.367505 \tValidation Loss: 0.526616\n",
      "Epoch: 95 \tTraining Loss: 0.339849 \tValidation Loss: 0.564500\n",
      "Epoch: 96 \tTraining Loss: 0.368201 \tValidation Loss: 0.524181\n",
      "Validation loss decreased (0.524965 --> 0.524181).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.460010 \tValidation Loss: 0.558834\n",
      "Epoch: 98 \tTraining Loss: 0.312997 \tValidation Loss: 0.558305\n",
      "Epoch: 99 \tTraining Loss: 0.388929 \tValidation Loss: 0.645981\n",
      "Epoch: 100 \tTraining Loss: 0.376316 \tValidation Loss: 0.549251\n",
      "Epoch: 101 \tTraining Loss: 0.340745 \tValidation Loss: 0.577354\n",
      "Epoch: 102 \tTraining Loss: 0.392984 \tValidation Loss: 0.785104\n",
      "Epoch: 103 \tTraining Loss: 0.405404 \tValidation Loss: 0.563262\n",
      "Epoch: 104 \tTraining Loss: 0.357175 \tValidation Loss: 0.468795\n",
      "Validation loss decreased (0.524181 --> 0.468795).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 0.295086 \tValidation Loss: 0.451711\n",
      "Validation loss decreased (0.468795 --> 0.451711).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 0.305029 \tValidation Loss: 0.517483\n",
      "Epoch: 107 \tTraining Loss: 0.303486 \tValidation Loss: 0.473512\n",
      "Epoch: 108 \tTraining Loss: 0.368868 \tValidation Loss: 0.477533\n",
      "Epoch: 109 \tTraining Loss: 0.388154 \tValidation Loss: 0.460312\n",
      "Epoch: 110 \tTraining Loss: 0.275989 \tValidation Loss: 0.408784\n",
      "Validation loss decreased (0.451711 --> 0.408784).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 0.295038 \tValidation Loss: 0.469210\n",
      "Epoch: 112 \tTraining Loss: 0.319592 \tValidation Loss: 0.640465\n",
      "Epoch: 113 \tTraining Loss: 0.314399 \tValidation Loss: 0.473870\n",
      "Epoch: 114 \tTraining Loss: 0.314086 \tValidation Loss: 0.505969\n",
      "Epoch: 115 \tTraining Loss: 0.354780 \tValidation Loss: 0.471647\n",
      "Epoch: 116 \tTraining Loss: 0.354941 \tValidation Loss: 0.507382\n",
      "Epoch: 117 \tTraining Loss: 0.316223 \tValidation Loss: 0.618590\n",
      "Epoch: 118 \tTraining Loss: 0.309657 \tValidation Loss: 0.392176\n",
      "Validation loss decreased (0.408784 --> 0.392176).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 0.321094 \tValidation Loss: 1.423431\n",
      "Epoch: 120 \tTraining Loss: 0.384571 \tValidation Loss: 0.457497\n",
      "Epoch: 121 \tTraining Loss: 0.327271 \tValidation Loss: 0.424360\n",
      "Epoch: 122 \tTraining Loss: 0.287499 \tValidation Loss: 0.366754\n",
      "Validation loss decreased (0.392176 --> 0.366754).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 0.267370 \tValidation Loss: 0.502578\n",
      "Epoch: 124 \tTraining Loss: 0.230476 \tValidation Loss: 0.350818\n",
      "Validation loss decreased (0.366754 --> 0.350818).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 0.257801 \tValidation Loss: 0.543130\n",
      "Epoch: 126 \tTraining Loss: 0.326634 \tValidation Loss: 0.405947\n",
      "Epoch: 127 \tTraining Loss: 0.253022 \tValidation Loss: 0.393913\n",
      "Epoch: 128 \tTraining Loss: 0.255999 \tValidation Loss: 0.318165\n",
      "Validation loss decreased (0.350818 --> 0.318165).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 0.232074 \tValidation Loss: 0.389612\n",
      "Epoch: 130 \tTraining Loss: 0.281921 \tValidation Loss: 0.446394\n",
      "Epoch: 131 \tTraining Loss: 0.228051 \tValidation Loss: 0.275180\n",
      "Validation loss decreased (0.318165 --> 0.275180).  Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 0.284707 \tValidation Loss: 0.340349\n",
      "Epoch: 133 \tTraining Loss: 0.212315 \tValidation Loss: 0.264709\n",
      "Validation loss decreased (0.275180 --> 0.264709).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 0.297736 \tValidation Loss: 0.567654\n",
      "Epoch: 135 \tTraining Loss: 0.278566 \tValidation Loss: 0.448157\n",
      "Epoch: 136 \tTraining Loss: 0.320495 \tValidation Loss: 0.279175\n",
      "Epoch: 137 \tTraining Loss: 0.251442 \tValidation Loss: 0.286847\n",
      "Epoch: 138 \tTraining Loss: 0.257136 \tValidation Loss: 0.306047\n",
      "Epoch: 139 \tTraining Loss: 0.186194 \tValidation Loss: 0.266947\n",
      "Epoch: 140 \tTraining Loss: 0.238405 \tValidation Loss: 0.283815\n",
      "Epoch: 141 \tTraining Loss: 0.223112 \tValidation Loss: 0.395065\n",
      "Epoch: 142 \tTraining Loss: 0.216889 \tValidation Loss: 0.186362\n",
      "Validation loss decreased (0.264709 --> 0.186362).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 0.203616 \tValidation Loss: 0.202549\n",
      "Epoch: 144 \tTraining Loss: 0.268344 \tValidation Loss: 0.197862\n",
      "Epoch: 145 \tTraining Loss: 0.248641 \tValidation Loss: 0.281187\n",
      "Epoch: 146 \tTraining Loss: 0.191441 \tValidation Loss: 0.243918\n",
      "Epoch: 147 \tTraining Loss: 0.229679 \tValidation Loss: 0.225042\n",
      "Epoch: 148 \tTraining Loss: 0.199182 \tValidation Loss: 0.231664\n",
      "Epoch: 149 \tTraining Loss: 0.193365 \tValidation Loss: 0.317799\n",
      "Epoch: 150 \tTraining Loss: 0.201982 \tValidation Loss: 0.238530\n",
      "Epoch: 151 \tTraining Loss: 0.165590 \tValidation Loss: 0.205620\n",
      "Epoch: 152 \tTraining Loss: 0.154687 \tValidation Loss: 0.263101\n",
      "Epoch: 153 \tTraining Loss: 0.225221 \tValidation Loss: 0.272523\n",
      "Epoch: 154 \tTraining Loss: 0.216537 \tValidation Loss: 0.265677\n",
      "Epoch: 155 \tTraining Loss: 0.213934 \tValidation Loss: 0.213674\n",
      "Epoch: 156 \tTraining Loss: 0.238337 \tValidation Loss: 0.173279\n",
      "Validation loss decreased (0.186362 --> 0.173279).  Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 0.129963 \tValidation Loss: 0.192674\n",
      "Epoch: 158 \tTraining Loss: 0.160390 \tValidation Loss: 0.198910\n",
      "Epoch: 159 \tTraining Loss: 0.211383 \tValidation Loss: 0.274868\n",
      "Epoch: 160 \tTraining Loss: 0.206454 \tValidation Loss: 0.265570\n",
      "Epoch: 161 \tTraining Loss: 0.156881 \tValidation Loss: 0.246035\n",
      "Epoch: 162 \tTraining Loss: 0.203682 \tValidation Loss: 0.153690\n",
      "Validation loss decreased (0.173279 --> 0.153690).  Saving model ...\n",
      "Epoch: 163 \tTraining Loss: 0.161752 \tValidation Loss: 0.181161\n",
      "Epoch: 164 \tTraining Loss: 0.196904 \tValidation Loss: 0.183744\n",
      "Epoch: 165 \tTraining Loss: 0.114569 \tValidation Loss: 0.184705\n",
      "Epoch: 166 \tTraining Loss: 0.147867 \tValidation Loss: 0.477737\n",
      "Epoch: 167 \tTraining Loss: 0.179224 \tValidation Loss: 0.297258\n",
      "Epoch: 168 \tTraining Loss: 0.169935 \tValidation Loss: 0.214474\n",
      "Epoch: 169 \tTraining Loss: 0.115030 \tValidation Loss: 0.236318\n",
      "Epoch: 170 \tTraining Loss: 0.125314 \tValidation Loss: 0.213762\n",
      "Epoch: 171 \tTraining Loss: 0.139516 \tValidation Loss: 0.181043\n",
      "Epoch: 172 \tTraining Loss: 0.118773 \tValidation Loss: 0.195574\n",
      "Epoch: 173 \tTraining Loss: 0.160813 \tValidation Loss: 0.144235\n",
      "Validation loss decreased (0.153690 --> 0.144235).  Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 0.176963 \tValidation Loss: 0.192635\n",
      "Epoch: 175 \tTraining Loss: 0.134373 \tValidation Loss: 0.159781\n",
      "Epoch: 176 \tTraining Loss: 0.106009 \tValidation Loss: 0.175172\n",
      "Epoch: 177 \tTraining Loss: 0.109909 \tValidation Loss: 0.205145\n",
      "Epoch: 178 \tTraining Loss: 0.123163 \tValidation Loss: 0.145746\n",
      "Epoch: 179 \tTraining Loss: 0.147958 \tValidation Loss: 0.263900\n",
      "Epoch: 180 \tTraining Loss: 0.134116 \tValidation Loss: 0.240313\n",
      "Epoch: 181 \tTraining Loss: 0.149256 \tValidation Loss: 0.273166\n",
      "Epoch: 182 \tTraining Loss: 0.106978 \tValidation Loss: 0.148031\n",
      "Epoch: 183 \tTraining Loss: 0.086415 \tValidation Loss: 0.189373\n",
      "Epoch: 184 \tTraining Loss: 0.141835 \tValidation Loss: 0.208790\n",
      "Epoch: 185 \tTraining Loss: 0.164348 \tValidation Loss: 0.216225\n",
      "Epoch: 186 \tTraining Loss: 0.250084 \tValidation Loss: 4.143953\n",
      "Epoch: 187 \tTraining Loss: 0.797780 \tValidation Loss: 0.278409\n",
      "Epoch: 188 \tTraining Loss: 0.160346 \tValidation Loss: 0.200995\n",
      "Epoch: 189 \tTraining Loss: 0.130523 \tValidation Loss: 0.189654\n",
      "Epoch: 190 \tTraining Loss: 0.136426 \tValidation Loss: 0.219111\n",
      "Epoch: 191 \tTraining Loss: 0.104598 \tValidation Loss: 0.181324\n",
      "Epoch: 192 \tTraining Loss: 0.127870 \tValidation Loss: 0.304593\n",
      "Epoch: 193 \tTraining Loss: 0.110602 \tValidation Loss: 0.153740\n",
      "Epoch: 194 \tTraining Loss: 0.112009 \tValidation Loss: 0.234855\n",
      "Epoch: 195 \tTraining Loss: 0.124269 \tValidation Loss: 0.132500\n",
      "Validation loss decreased (0.144235 --> 0.132500).  Saving model ...\n",
      "Epoch: 196 \tTraining Loss: 0.126841 \tValidation Loss: 0.238554\n",
      "Epoch: 197 \tTraining Loss: 0.166993 \tValidation Loss: 0.139539\n",
      "Epoch: 198 \tTraining Loss: 0.107764 \tValidation Loss: 0.208546\n",
      "Epoch: 199 \tTraining Loss: 0.141646 \tValidation Loss: 0.145869\n",
      "Epoch: 200 \tTraining Loss: 0.112916 \tValidation Loss: 0.239041\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model_scratch = train(200, loaders_scratch, model_scratch, optimizer_scratch, \n",
    "                      criterion_scratch, use_cuda, 'model_scratch.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Test the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model_scratch.load_state_dict(torch.load('model_scratch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.163932\n",
      "\n",
      "\n",
      "Test Accuracy: 92% (52/56)\n"
     ]
    }
   ],
   "source": [
    "# call test function    \n",
    "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
